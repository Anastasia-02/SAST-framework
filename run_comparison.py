#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —ç—Ç–∞–ª–æ–Ω–æ–º.
–í—ã–≤–æ–¥–∏—Ç –æ—Ç—á—ë—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: –ø–æ–ª–Ω–æ—Ç–∞, –¥–µ–ª—å—Ç–∞ FP, –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞.
"""

import sys
import json
import logging
import argparse
from pathlib import Path
from datetime import datetime

# –°–æ–∑–¥–∞—ë–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
Path("logs").mkdir(exist_ok=True)
Path("results/comparison").mkdir(parents=True, exist_ok=True)
Path("results/metrics").mkdir(parents=True, exist_ok=True)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
log_filename = f"logs/comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ –ø—É—Ç—å Python
root_dir = Path(__file__).parent
sys.path.insert(0, str(root_dir))

# –ò–º–ø–æ—Ä—Ç—ã –º–æ–¥—É–ª–µ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞
try:
    from test_runner import TestRunner
    from comparer import Comparer
    from performance_metrics import PerformanceCollector
    import yaml
except ImportError as e:
    logger.error(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞: {e}")
    print("‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã: pip install pyyaml docker")
    sys.exit(1)

def main(force_baseline: bool = False):
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")

    # –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–æ–≤
    print("\nüìä –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–æ–≤...")
    config_path = "config/projects_config.yaml"
    if not Path(config_path).exists():
        logger.error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
        sys.exit(1)

    runner = TestRunner(config_path)
    test_results = runner.run_all_tests()

    if not test_results:
        logger.warning("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        return

    # –®–∞–≥ 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —ç—Ç–∞–ª–æ–Ω–æ–º
    print("\nüìä –®–∞–≥ 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —ç—Ç–∞–ª–æ–Ω–æ–º...")
    comparer = Comparer()
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    comparison_results = comparer.compare_all(config)

    if not comparison_results:
        logger.warning("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    else:
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–æ–≤ (JSON)
        summary = comparer.generate_summary_report()
        comparer.generate_detailed_report()

        # –í—ã–≤–æ–¥ —Å–≤–æ–¥–∫–∏ —Å —Ç—Ä–µ–º—è –∫–ª—é—á–µ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        print("\nüìã –°–≤–æ–¥–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (–∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏):")
        print("=" * 80)
        for project_name, project_results in comparison_results.items():
            print(f"\nüìÅ –ü—Ä–æ–µ–∫—Ç: {project_name}")
            print("-" * 60)
            for tool_name, result in project_results.items():
                recall = result.metrics.get('recall_percentage', 0)
                fp_delta = result.metrics.get('fp_delta', 0)
                f1 = result.metrics.get('f1_score', 0)

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–∑ test_results
                exec_time = None
                if project_name in test_results and tool_name in test_results[project_name]:
                    perf = test_results[project_name][tool_name].get('performance')
                    if perf:
                        # PerformanceMetrics –º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±—ä–µ–∫—Ç–æ–º –∏–ª–∏ dict
                        if hasattr(perf, 'execution_time'):
                            exec_time = perf.execution_time
                        elif isinstance(perf, dict):
                            exec_time = perf.get('execution_time')
                        else:
                            exec_time = None

                status = "‚úÖ" if recall >= 90 else "‚ö†Ô∏è" if recall >= 70 else "‚ùå"
                print(f"   {status} –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {tool_name}")
                print(f"      üìä –ü–æ–ª–Ω–æ—Ç–∞ (recall): {recall:.1f}%")
                fp_display = f"{fp_delta:+d}" if fp_delta != 0 else "0"
                print(f"      üìà –î–µ–ª—å—Ç–∞ FP: {fp_display}")
                if exec_time is not None:
                    print(f"      ‚è±Ô∏è  –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {exec_time:.2f} —Å–µ–∫")
                else:
                    print(f"      ‚è±Ô∏è  –í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: N/A")
                print(f"      üîç –°–æ–≤–ø–∞–¥–µ–Ω–∏–π: {result.matched_issues}/{result.baseline_issues}")
                print(f"      üÜï –ù–æ–≤—ã–µ: {result.new_issues}, üö´ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ: {result.missing_issues}")
                print(f"      ‚öñÔ∏è  F1-–º–µ—Ä–∞: {f1:.3f}")
                print()

        # –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
        print("\nüìä –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫:")
        print("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
        print("‚îÇ –ü—Ä–æ–µ–∫—Ç              ‚îÇ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç   ‚îÇ –ü–æ–ª–Ω–æ—Ç–∞(%) ‚îÇ –î–µ–ª—å—Ç–∞ FP   ‚îÇ –í—Ä–µ–º—è (—Å–µ–∫) ‚îÇ")
        print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")

        for project_name, project_results in comparison_results.items():
            for tool_name, result in project_results.items():
                recall = f"{result.metrics.get('recall_percentage', 0):.1f}"
                fp_delta_val = result.metrics.get('fp_delta', 0)
                fp = f"{fp_delta_val:+d}" if fp_delta_val != 0 else "0"
                exec_time = "N/A"
                if project_name in test_results and tool_name in test_results[project_name]:
                    perf = test_results[project_name][tool_name].get('performance')
                    if perf:
                        if hasattr(perf, 'execution_time'):
                            exec_time = f"{perf.execution_time:.2f}"
                        elif isinstance(perf, dict) and 'execution_time' in perf:
                            exec_time = f"{perf['execution_time']:.2f}"
                # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ –∏–º–µ–Ω–∞, —á—Ç–æ–±—ã —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ —Ä–∞–∑—ä–µ–∑–∂–∞–ª–∞—Å—å
                project_short = project_name[:19] if len(project_name) > 19 else project_name
                tool_short = tool_name[:12] if len(tool_name) > 12 else tool_name
                print(f"‚îÇ {project_short:<19} ‚îÇ {tool_short:<12} ‚îÇ {recall:>10} ‚îÇ {fp:>11} ‚îÇ {exec_time:>11} ‚îÇ")

        print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")

    # –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print("\nüìä –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    perf_collector = PerformanceCollector()
    perf_report = perf_collector.generate_performance_report()

    if perf_report and 'tools_performance' in perf_report:
        print("\n‚è±Ô∏è  –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—Å—Ä–µ–¥–Ω–∏–µ –ø–æ –≤—Å–µ–º –∑–∞–ø—É—Å–∫–∞–º):")
        print("=" * 60)
        for tool, metrics in perf_report['tools_performance'].items():
            avg_time = metrics.get('avg_execution_time', 0)
            avg_issues = metrics.get('avg_issues_found', 0)
            print(f"   üõ†Ô∏è  {tool}:")
            print(f"      –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {avg_time:.2f} —Å–µ–∫")
            print(f"      –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π: {avg_issues:.1f}")
            if avg_time > 0:
                print(f"      –°–∫–æ—Ä–æ—Å—Ç—å: {avg_issues/avg_time:.2f} —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π/—Å–µ–∫")
        print("=" * 60)

    # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞
    print("\nüìä –®–∞–≥ 4: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á—ë—Ç–∞...")
    final_report = {
        "timestamp": datetime.now().isoformat(),
        "test_results_summary": {},
        "comparison_summary": {},
        "performance_summary": perf_report.get('tools_performance', {}) if perf_report else {}
    }
    for project_name, tools_results in test_results.items():
        final_report["test_results_summary"][project_name] = {}
        for tool_name, data in tools_results.items():
            perf = data.get('performance')
            exec_time = None
            if perf:
                if hasattr(perf, 'execution_time'):
                    exec_time = perf.execution_time
                elif isinstance(perf, dict):
                    exec_time = perf.get('execution_time')
            final_report["test_results_summary"][project_name][tool_name] = {
                "success": data.get('success', False),
                "issues_count": data.get('issues_count', 0),
                "execution_time": exec_time
            }
    if comparison_results:
        for project_name, project_results in comparison_results.items():
            final_report["comparison_summary"][project_name] = {}
            for tool_name, result in project_results.items():
                final_report["comparison_summary"][project_name][tool_name] = {
                    "recall_percentage": result.metrics.get('recall_percentage', 0),
                    "fp_delta": result.metrics.get('fp_delta', 0),
                    "matched": result.matched_issues,
                    "new": result.new_issues,
                    "missing": result.missing_issues,
                    "f1_score": result.metrics.get('f1_score', 0)
                }

    final_report_path = "results/final_report.json"
    with open(final_report_path, 'w', encoding='utf-8') as f:
        json.dump(final_report, f, indent=2, ensure_ascii=False)
    print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {final_report_path}")
    print(f"üìù –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª–µ: {log_filename}")
    print("\nüéâ –ü—Ä–æ—Ü–µ—Å—Å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
    parser.add_argument("--force", action="store_true", help="–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å baseline –ø–µ—Ä–µ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ–º")
    args = parser.parse_args()
    main(force_baseline=args.force)