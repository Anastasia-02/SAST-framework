#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å —ç—Ç–∞–ª–æ–Ω–æ–º
"""

import sys
import json
from pathlib import Path
import logging
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å Python
sys.path.insert(0, str(Path(__file__).parent))


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""

    # 1. –°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–µ–º –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    print("üìÅ –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
    Path("logs").mkdir(exist_ok=True)
    Path("results/comparison").mkdir(parents=True, exist_ok=True)
    Path("results/metrics").mkdir(parents=True, exist_ok=True)
    Path("results/raw").mkdir(parents=True, exist_ok=True)
    Path("results/normalized").mkdir(parents=True, exist_ok=True)
    Path("baseline").mkdir(exist_ok=True)

    # 2. –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ü–û–°–õ–ï —Å–æ–∑–¥–∞–Ω–∏—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    log_filename = f"logs/comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename),
            logging.StreamHandler()
        ]
    )

    logger = logging.getLogger(__name__)

    print(f"üìù –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ: {log_filename}")
    print("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")

    # 3. –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª–∏ –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    try:
        from test_runner import TestRunner
        from comparer import Comparer
        from performance_metrics import PerformanceCollector
        import yaml

    except ImportError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π: {e}")
        print(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã:")
        print("pip install pyyaml")
        sys.exit(1)

    try:
        # 1. –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        print("\nüìä –®–∞–≥ 1: –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–æ–≤...")
        config_path = "config/projects_config.yaml"

        if not Path(config_path).exists():
            logger.error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
            print(f"‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_path}")
            print("–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª config/projects_config.yaml")
            sys.exit(1)

        runner = TestRunner(config_path)
        test_results = runner.run_all_tests()

        if not test_results:
            logger.warning("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
            print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")
        else:
            logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü—Ä–æ–µ–∫—Ç–æ–≤: {len(test_results)}")
            print(f"‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ü—Ä–æ–µ–∫—Ç–æ–≤: {len(test_results)}")

            # –í—ã–≤–æ–¥–∏–º –∫—Ä–∞—Ç–∫—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            for project_name, tools_results in test_results.items():
                success_tools = [t for t, r in tools_results.items() if r.get('success')]
                issues_total = sum(r.get('issues_count', 0) for r in tools_results.values() if r.get('success'))
                print(
                    f"   {project_name}: {len(success_tools)}/{len(tools_results)} –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, {issues_total} —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}", exc_info=True)
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        sys.exit(1)

    # 2. –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å —ç—Ç–∞–ª–æ–Ω–æ–º
    print("\nüìä –®–∞–≥ 2: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —ç—Ç–∞–ª–æ–Ω–æ–º...")
    comparer = Comparer()

    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–æ–≤
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)

        comparison_results = comparer.compare_all(config)

        if not comparison_results:
            logger.warning("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            print("‚ö†Ô∏è  –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
        else:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã
            summary = comparer.generate_summary_report()
            comparer.generate_detailed_report()

            print("\nüìã –°–≤–æ–¥–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
            print("=" * 60)

            for project_name, project_results in comparison_results.items():
                print(f"\nüìÅ –ü—Ä–æ–µ–∫—Ç: {project_name}")
                print("-" * 40)

                for tool_name, result in project_results.items():
                    recall_pct = result.metrics.get('recall_percentage', 0)
                    fp_delta = result.metrics.get('fp_delta', 0)
                    f1_score = result.metrics.get('f1_score', 0)

                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                    if recall_pct >= 90:
                        status = "‚úÖ"
                    elif recall_pct >= 70:
                        status = "‚ö†Ô∏è "
                    else:
                        status = "‚ùå"

                    print(f"   {status} –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç: {tool_name}")
                    print(f"      üìä –ü–æ–ª–Ω–æ—Ç–∞ (recall): {recall_pct:.1f}%")
                    print(f"      üîç –°–æ–≤–ø–∞–¥–µ–Ω–∏–π: {result.matched_issues}/{result.baseline_issues}")
                    print(f"      üÜï –ù–æ–≤—ã–µ: {result.new_issues}")
                    print(f"      üö´ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ: {result.missing_issues}")
                    print(f"      üìà –î–µ–ª—å—Ç–∞ FP: {fp_delta:+d}")
                    print(f"      ‚öñÔ∏è  F1-–º–µ—Ä–∞: {f1_score:.3f}")
                    print(f"      üìÅ Baseline: {result.baseline_issues}, Current: {result.current_issues}")
                    print()

            print("=" * 60)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏: {e}", exc_info=True)
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏: {e}")

    # 3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    print("\nüìä –®–∞–≥ 3: –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏...")
    perf_collector = PerformanceCollector()

    try:
        perf_report = perf_collector.generate_performance_report()

        if perf_report and 'tools_performance' in perf_report:
            print("\n‚è±Ô∏è  –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
            print("=" * 60)

            for tool, metrics in perf_report['tools_performance'].items():
                avg_time = metrics.get('avg_execution_time', 0)
                avg_issues = metrics.get('avg_issues_found', 0)
                issues_per_sec = avg_issues / avg_time if avg_time > 0 else 0

                print(f"\n   üõ†Ô∏è  {tool}:")
                print(f"      –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.1f} —Å–µ–∫")
                print(f"      –°—Ä–µ–¥–Ω–µ–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π: {avg_issues:.0f}")
                print(f"      –°–∫–æ—Ä–æ—Å—Ç—å: {issues_per_sec:.1f} —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π/—Å–µ–∫")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–π –∏ —Ö—É–¥—à–∏–π –∑–∞–ø—É—Å–∫
                best = metrics.get('best_run', {})
                worst = metrics.get('worst_run', {})

                if best:
                    print(f"      üèÜ –õ—É—á—à–∏–π –∑–∞–ø—É—Å–∫: {best.get('execution_time', 0):.1f} —Å–µ–∫")
                if worst:
                    print(f"      üêå –•—É–¥—à–∏–π –∑–∞–ø—É—Å–∫: {worst.get('execution_time', 0):.1f} —Å–µ–∫")

            print("=" * 60)
        else:
            print("‚ÑπÔ∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = perf_report.get('recommendations', []) if perf_report else []
        if recommendations:
            print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:")
            for rec in recommendations:
                severity = rec.get('severity', 'info').upper()
                message = rec.get('message', '')
                print(f"   [{severity}] {message}")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –º–µ—Ç—Ä–∏–∫: {e}", exc_info=True)
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –º–µ—Ç—Ä–∏–∫: {e}")

    # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç
    print("\nüìä –®–∞–≥ 4: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
    try:
        final_report = {
            "timestamp": datetime.now().isoformat(),
            "test_results_summary": {},
            "comparison_summary": {},
            "performance_summary": {},
            "metadata": {
                "framework_version": "1.0.0",
                "execution_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
        }

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é
        for project_name, tools_results in test_results.items():
            final_report["test_results_summary"][project_name] = {}

            for tool_name, data in tools_results.items():
                final_report["test_results_summary"][project_name][tool_name] = {
                    "success": data.get('success', False),
                    "issues_count": data.get('issues_count', 0),
                    "has_error": 'error' in data,
                    "error_message": data.get('error', None)
                }

        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é
        if comparison_results:
            for project_name, project_results in comparison_results.items():
                final_report["comparison_summary"][project_name] = {}

                for tool_name, result in project_results.items():
                    final_report["comparison_summary"][project_name][tool_name] = {
                        "recall_percentage": result.metrics.get('recall_percentage', 0),
                        "fp_delta": result.metrics.get('fp_delta', 0),
                        "matched": result.matched_issues,
                        "new": result.new_issues,
                        "missing": result.missing_issues,
                        "f1_score": result.metrics.get('f1_score', 0),
                        "status": "good" if result.metrics.get('recall_percentage', 0) >= 90 else
                        "warning" if result.metrics.get('recall_percentage', 0) >= 70 else
                        "bad"
                    }

        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if perf_report and 'tools_performance' in perf_report:
            final_report["performance_summary"] = {}

            for tool, metrics in perf_report['tools_performance'].items():
                final_report["performance_summary"][tool] = {
                    "avg_execution_time": metrics.get('avg_execution_time', 0),
                    "avg_issues_found": metrics.get('avg_issues_found', 0),
                    "total_runs": metrics.get('total_runs', 0)
                }

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        final_report_path = "results/final_report.json"
        with open(final_report_path, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {final_report_path}")

        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É
        print("\nüìà –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:")
        print("=" * 60)

        total_projects = len(final_report.get("test_results_summary", {}))
        successful_tools = 0
        total_tools = 0

        for project, tools in final_report.get("test_results_summary", {}).items():
            for tool, data in tools.items():
                total_tools += 1
                if data.get("success"):
                    successful_tools += 1

        print(f"   –ü—Ä–æ–µ–∫—Ç–æ–≤ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ: {total_projects}")
        print(f"   –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ: {successful_tools}/{total_tools}")

        if comparison_results:
            avg_recall = 0
            count = 0
            for project, tools in final_report.get("comparison_summary", {}).items():
                for tool, data in tools.items():
                    avg_recall += data.get("recall_percentage", 0)
                    count += 1

            if count > 0:
                avg_recall /= count
                print(f"   –°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞: {avg_recall:.1f}%")

        print("=" * 60)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {e}", exc_info=True)
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {e}")

    print("\nüéâ –ü—Ä–æ—Ü–µ—Å—Å —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: results/")
    print(f"üìù –õ–æ–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª–µ: {log_filename}")


if __name__ == "__main__":
    main()